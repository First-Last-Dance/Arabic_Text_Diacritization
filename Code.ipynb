{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","In this notebook, we will create a LSTM seq2seq model for Arabic text diacritization. The goal is to add diacritics to the Arabic text, which can help in pronunciation and understanding.\n"]},{"cell_type":"markdown","metadata":{},"source":["## Libraries\n","\n","Importing the necessary libraries, including Pytorch for building and training our model."]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:55:46.082783Z","iopub.status.busy":"2024-01-02T06:55:46.082032Z","iopub.status.idle":"2024-01-02T06:56:00.900363Z","shell.execute_reply":"2024-01-02T06:56:00.899544Z","shell.execute_reply.started":"2024-01-02T06:55:46.082733Z"},"id":"sEjy1oTo5x5m","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import re\n","import matplotlib.pyplot as plt\n","import math\n","import statistics\n","import nltk\n","from nltk import word_tokenize\n","import gensim\n","from gensim.models import Word2Vec\n","import multiprocessing\n","from tashaphyne import normalize\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","import numpy as np\n","import pickle\n","import os\n","import pandas as pd\n","from tqdm import tqdm\n","import torch\n","from torch import nn\n","import random as rnd\n","from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-01-02T06:56:00.902076Z","iopub.status.busy":"2024-01-02T06:56:00.901575Z","iopub.status.idle":"2024-01-02T06:56:01.030620Z","shell.execute_reply":"2024-01-02T06:56:01.029580Z","shell.execute_reply.started":"2024-01-02T06:56:00.902047Z"},"id":"l-bfpdtoSYr4","outputId":"507bb6eb-0191-4a47-d4d9-ad0d1a984d79","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('punkt')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:56:08.225644Z","iopub.status.busy":"2024-01-02T06:56:08.225234Z","iopub.status.idle":"2024-01-02T06:56:08.230455Z","shell.execute_reply":"2024-01-02T06:56:08.229447Z","shell.execute_reply.started":"2024-01-02T06:56:08.225612Z"},"id":"Rb_QXa_o5x5r","trusted":true},"outputs":[],"source":["class featureType():\n","    __slots__ = ('BagOfWords','TF_IDF')"]},{"cell_type":"markdown","metadata":{},"source":["# Data preprocessing"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:56:09.864047Z","iopub.status.busy":"2024-01-02T06:56:09.863298Z","iopub.status.idle":"2024-01-02T06:56:09.874452Z","shell.execute_reply":"2024-01-02T06:56:09.873344Z","shell.execute_reply.started":"2024-01-02T06:56:09.864012Z"},"id":"wa4pvQro5x5v","trusted":true},"outputs":[],"source":["def preprocessing(data, maxSentenceLength = 20):\n","    # Lists to store processed data\n","    tokenized_sentences = []\n","    tokenized_sentences_with_diacritics = []\n","    vocab = set()  # Set to store unique tokens\n","\n","    # Preprocessing steps\n","    pre_data = re.sub(r'\\n',' \\n',data)\n","    pre_data = re.sub(r'[,،;!~\\\"*\\d\\(\\){}\\[\\]/\\\\\\'«»`\\-\\–(\\u200f)]','',pre_data)\n","    pre_data = re.sub(r'\\s+',' ',pre_data)\n","    sentenses = re.split(r'[؛.:؟\\n]',pre_data)\n","    last_sentences = []\n","    \n","    # Loop through each sentence\n","    for i in range(len(sentenses)):\n","        words = sentenses[i].split()\n","        # Break long sentences into chunks with a maximum length of `maxSentenceLength`\n","        new_sentences = [\" \".join(words[i:i+maxSentenceLength]) for i in range(0, len(words), maxSentenceLength)]\n","        \n","        # Process each chunk\n","        for sentence in new_sentences:\n","            # Text normalization\n","            sentence = normalize.strip_tatweel(sentence)\n","            sentence = normalize.normalize_lamalef(sentence)\n","            \n","            # Tokenize the sentence with diacritics\n","            tokens_with_diacritics = word_tokenize(sentence)\n","            tokenized_sentences_with_diacritics.append(tokens_with_diacritics)\n","            \n","            # Remove diacritics\n","            sentence = normalize.strip_tashkeel(sentence)\n","            last_sentences.append(sentence.strip())\n","            \n","            # Tokenize the sentence without diacritics\n","            tokens = word_tokenize(sentence)\n","            tokenized_sentences.append(tokens)\n","            \n","            # Update vocabulary set with unique tokens\n","            vocab.update(tokens)\n","            \n","    # Return processed data\n","    return last_sentences, tokenized_sentences, tokenized_sentences_with_diacritics, vocab"]},{"cell_type":"markdown","metadata":{},"source":["# Feature extraction"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:56:11.743928Z","iopub.status.busy":"2024-01-02T06:56:11.743252Z","iopub.status.idle":"2024-01-02T06:56:11.750982Z","shell.execute_reply":"2024-01-02T06:56:11.749817Z","shell.execute_reply.started":"2024-01-02T06:56:11.743889Z"},"id":"VSFbbf2U5x5w","trusted":true},"outputs":[],"source":["def featureExtraction(tokenized_sentences, sentences, vocab, type = featureType.TF_IDF):\n","    # Lists to store extracted features and words\n","    features = []\n","    words = []\n","    \n","    # Check the feature type requested\n","    if type is featureType.BagOfWords:\n","        print(\"Bag Of Words\")\n","        # Create a Bag of Words model\n","        model = CountVectorizer()\n","        # Fit the model on the tokenized sentences and transform into a feature matrix\n","        features = model.fit_transform(sentences).toarray()\n","        # Get the feature names (words)\n","        words = model.get_feature_names_out()\n","    elif type is featureType.TF_IDF:\n","        print(\"TF-IDF\")\n","        # Create a TF-IDF model\n","        model = TfidfVectorizer()\n","        # Fit the model on the tokenized sentences and transform into a feature matrix\n","        features = model.fit_transform(sentences).toarray()\n","        # Get the feature names (words)\n","        words = model.get_feature_names_out()\n","\n","    # Return the model, features, and words\n","    return model, features, words"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:56:14.458207Z","iopub.status.busy":"2024-01-02T06:56:14.457547Z","iopub.status.idle":"2024-01-02T06:56:14.464999Z","shell.execute_reply":"2024-01-02T06:56:14.463932Z","shell.execute_reply.started":"2024-01-02T06:56:14.458173Z"},"id":"g8wsUwsT5x5w","trusted":true},"outputs":[],"source":["def wordIndexer(tokenized_sentences, vocab, sentenceLength=20):\n","    # Convert vocabulary set to a list\n","    vocab_list = list(vocab)\n","    # Create a dictionary to map each word to its index in the vocabulary list\n","    vocab_dict = {item: index for index, item in enumerate(vocab_list)}\n","    # List to store indexed sentences\n","    sentences_indexer = []\n","\n","    # Iterate through each tokenized sentence\n","    for sentence in tokenized_sentences:\n","        sentence_indexer = []\n","\n","        # Map each word to its index in the vocabulary or use a special index for out-of-vocabulary words\n","        for i in range(len(sentence)):\n","            sentence_indexer.append(vocab_dict.get(sentence[i], len(vocab) + 3))\n","\n","        # Pad the sentence with a special index if its length is less than `sentenceLength`\n","        for i in range(len(sentence), sentenceLength, 1):\n","            sentence_indexer.append(len(vocab) + 2)\n","\n","        # Add the indexed sentence to the list\n","        sentences_indexer.append(sentence_indexer)\n","\n","    # Return the list of indexed sentences\n","    return sentences_indexer"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:56:14.641922Z","iopub.status.busy":"2024-01-02T06:56:14.641614Z","iopub.status.idle":"2024-01-02T06:56:14.650885Z","shell.execute_reply":"2024-01-02T06:56:14.649987Z","shell.execute_reply.started":"2024-01-02T06:56:14.641893Z"},"id":"JMkvhcjL5x5w","trusted":true},"outputs":[],"source":["def charIndexer(tokenized_sentences_with_diacritics, max_length, diacritics, arabic_letters):\n","    # Lists to store indexed characters and diacritics for each sentence\n","    sentences_chars = []\n","    sentences_diacritics = []\n","\n","    # Iterate through each tokenized sentence with diacritics\n","    for sentence in tokenized_sentences_with_diacritics:\n","        chars = []  # List to store indexed characters for the current sentence\n","        diacs = []  # List to store indexed diacritics for the current sentence\n","\n","        # Iterate through each word in the sentence\n","        for word in sentence:\n","            for i in range(len(word)):\n","                if word[i] not in diacritics:\n","                    # Map the character to its index in `arabic_letters`\n","                    chars.append(arabic_letters[word[i]])\n","\n","                    # Check for diacritics and map them to their indices in `diacritics`\n","                    if i + 2 < len(word) and word[i+1:i+3] in diacritics:\n","                        diacs.append(diacritics[word[i+1:i+3]])\n","                        i += 2\n","                    elif i + 1 < len(word) and word[i+1] in diacritics:\n","                        diacs.append(diacritics[word[i+1]])\n","                        i += 1\n","                    else:\n","                        diacs.append(diacritics[''])\n","\n","        # Pad the sentence with special indices if its length is less than `max_length`\n","        for i in range(len(chars), max_length):\n","            chars.append(arabic_letters['$'])\n","            diacs.append(diacritics[''])\n","\n","        # Add the indexed characters and diacritics to the respective lists\n","        sentences_chars.append(chars)\n","        sentences_diacritics.append(diacs)\n","\n","    # Return the lists of indexed characters and diacritics\n","    return sentences_chars, sentences_diacritics"]},{"cell_type":"markdown","metadata":{},"source":["# ArabicDataset\n","The class that impelements the dataset for the model"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:56:14.822296Z","iopub.status.busy":"2024-01-02T06:56:14.821580Z","iopub.status.idle":"2024-01-02T06:56:14.832685Z","shell.execute_reply":"2024-01-02T06:56:14.831737Z","shell.execute_reply.started":"2024-01-02T06:56:14.822268Z"},"id":"KNKDooHIz-MV","trusted":true},"outputs":[],"source":["class ArabicDatasetTFIDF(torch.utils.data.Dataset):\n","\n","  def __init__(self, tokenized_sentences, features, sentences_chars, sentences_diacritics, words, maxLength = 130):\n","    self.tokenized_sentences = tokenized_sentences\n","    self.features = features\n","    self.sentences_chars = torch.tensor(sentences_chars, dtype=torch.float32)\n","    self.sentences_diacritics = torch.tensor(sentences_diacritics)\n","    self.words = words\n","    self.maxLength = maxLength\n","\n","  def __len__(self):\n","    return len(self.sentences_diacritics)\n","\n","  def __getitem__(self, idx):\n","    sentence_tokens_features = []\n","    for i in range(len(self.tokenized_sentences[idx])):\n","      if self.tokenized_sentences[idx][i] in self.words:\n","        ind = np.where(self.words == self.tokenized_sentences[idx][i])[0][0]\n","        sentence_tokens_features.append(list(self.features[:,ind]))\n","      else:\n","        sentence_tokens_features.append(list([1]*(self.features.shape[0])))\n","    for i in range(len(self.tokenized_sentences[idx]),self.maxLength):\n","      sentence_tokens_features.append(list([1]*(self.features.shape[0])))\n","\n","    return torch.tensor(sentence_tokens_features), self.sentences_chars[idx,:], self.sentences_diacritics[idx,:]\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:56:14.967054Z","iopub.status.busy":"2024-01-02T06:56:14.966763Z","iopub.status.idle":"2024-01-02T06:56:14.974337Z","shell.execute_reply":"2024-01-02T06:56:14.973357Z","shell.execute_reply.started":"2024-01-02T06:56:14.967028Z"},"id":"uFOC4Kqm5x5x","trusted":true},"outputs":[],"source":["class ArabicDataset(torch.utils.data.Dataset):\n","\n","  def __init__(self, vocab_size,sentences_indexer, sentences_chars, sentences_diacritics):\n","    self.sentences_indexer = torch.tensor(sentences_indexer)\n","    self.sentences_chars = torch.tensor(sentences_chars)\n","    self.sentences_diacritics = torch.tensor(sentences_diacritics)\n","    self.vocab_size = vocab_size\n","\n","\n","  def __len__(self):\n","    return len(self.sentences_indexer)\n","\n","  def __getitem__(self, idx):\n","    \n","    return self.sentences_indexer[idx], self.sentences_chars[idx], self.sentences_diacritics[idx]"]},{"cell_type":"markdown","metadata":{},"source":["# Arabic Diacritization Model\n","The class that implements the pytorch model"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:56:15.053372Z","iopub.status.busy":"2024-01-02T06:56:15.053092Z","iopub.status.idle":"2024-01-02T06:56:15.061798Z","shell.execute_reply":"2024-01-02T06:56:15.060818Z","shell.execute_reply.started":"2024-01-02T06:56:15.053347Z"},"id":"ZpsL4W4f5x5y","trusted":true},"outputs":[],"source":["class ArabicDiacritization(nn.Module):\n","  def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, dropout,n_classes = 15):\n","    super(ArabicDiacritization, self).__init__()\n","\n","    # Word embedding layer\n","    self.embedding_word = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim)\n","\n","    # Bidirectional LSTM for words\n","    self.lstm_words = nn.LSTM(\n","                    input_size=embedding_dim,\n","                    hidden_size=hidden_size,\n","                    batch_first=True,\n","                    bidirectional=True,\n","                    num_layers=num_layers,\n","                    dropout = dropout\n","                    )\n","    \n","    # Character embedding layer\n","    self.embedding_char = nn.Embedding(num_embeddings=37, embedding_dim=70)\n","    \n","    # Bidirectional LSTM for characters\n","    self.lstm_chars = nn.LSTM(\n","                    input_size = 70,\n","                    hidden_size=hidden_size,\n","                    batch_first=True,\n","                    bidirectional=True,\n","                    num_layers=num_layers,\n","                    dropout = dropout\n","                    )\n","\n","    # Linear layer for final classification\n","    self.linear = nn.Linear(in_features = hidden_size * 2, out_features=n_classes)\n","\n","  def forward(self, sentences, chars):\n","\n","    # Word embedding\n","    embedding_word = self.embedding_word(sentences)\n","    \n","    # Character embedding\n","    embedding_cahr = self.embedding_char(chars)\n","    \n","    # LSTM for words\n","    word_out, (h,c) = self.lstm_words(embedding_word)\n","\n","    # LSTM for characters using the hidden states from the word LSTM\n","    char_out, _ = self.lstm_chars(embedding_cahr,(h,c))\n","    \n","    # Final linear layer for classification\n","    final_output = self.linear(char_out)\n","\n","    return final_output"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:56:15.740092Z","iopub.status.busy":"2024-01-02T06:56:15.738943Z","iopub.status.idle":"2024-01-02T06:56:15.753953Z","shell.execute_reply":"2024-01-02T06:56:15.752830Z","shell.execute_reply.started":"2024-01-02T06:56:15.740054Z"},"id":"f5U_gwbA5x5z","trusted":true},"outputs":[],"source":["def training(model, train_dataset, pad, batch_size=512, epochs=10, learning_rate=0.01):\n","\n","  # Create a DataLoader for the training dataset\n","  train_dataloader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True)\n","\n","  # Define the CrossEntropyLoss criterion\n","  criterion = torch.nn.CrossEntropyLoss()\n","\n","  # Define the Adam optimizer\n","  optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, )\n","\n","  # Check if CUDA (GPU) is available\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  \n","  if use_cuda:\n","    print(\"Cuda\")\n","    # Move the model and criterion to GPU\n","    model = model.cuda()\n","    criterion = criterion.cuda()\n","\n","  # Convert the padding value to a tensor and move it to the appropriate device\n","  pad = torch.tensor(pad)\n","  pad = pad.to(device)\n","\n","  # Loop through epochs\n","  for epoch_num in range(epochs):\n","    total_acc_train = 0\n","    total_loss_train = 0\n","\n","    # Lists to store output and predictions\n","    f_output = []\n","    total_predection = 0\n","\n","    # Loop through batches in the training dataloader\n","    for sentences_indexer, sentences_char, sentences_diacritics in tqdm(train_dataloader):\n","      \n","      # Move data to the appropriate device (CPU or GPU)\n","      sentences_indexer = sentences_indexer.to(device)\n","      sentences_char = sentences_char.to(device)\n","      sentences_diacritics = sentences_diacritics.to(device)\n","\n","      # Forward pass\n","      output = model(sentences_indexer,sentences_char)\n","      output = output.to(device)\n","        \n","      # Reshape tensors for calculation of loss\n","      sentences_char = sentences_char.view(-1)\n","      output = output.view(-1, output.size(-1))\n","      sentences_diacritics = sentences_diacritics.view(-1)\n","\n","      # Apply mask to exclude padding values\n","      mask = sentences_char != pad\n","      sentences_char = sentences_char[mask]\n","      output = output[mask]\n","      sentences_diacritics = sentences_diacritics[mask]\n","\n","      # Calculate batch loss\n","      batch_loss = criterion(output,sentences_diacritics)\n","      total_loss_train += batch_loss\n","\n","      # Calculate batch accuracy\n","      acc = (sentences_diacritics == torch.argmax(output,dim=1)).sum().item()\n","      total_acc_train += acc\n","      total_predection += sentences_diacritics.size(0)\n","\n","      # Backward pass and optimization step\n","      optimizer.zero_grad()\n","      batch_loss.backward()\n","      optimizer.step()\n","\n","    # Calculate average loss and accuracy for the epoch\n","    epoch_loss = total_loss_train / len(train_dataset)\n","    epoch_acc = total_acc_train / total_predection\n","\n","    # Print epoch summary\n","    print(\n","        f'Epochs: {epoch_num + 1} | Train Loss: {epoch_loss} \\\n","        | Train Accuracy: {epoch_acc}\\n')\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-01-02T06:56:16.803430Z","iopub.status.busy":"2024-01-02T06:56:16.803032Z","iopub.status.idle":"2024-01-02T06:56:16.886174Z","shell.execute_reply":"2024-01-02T06:56:16.885183Z","shell.execute_reply.started":"2024-01-02T06:56:16.803377Z"},"id":"3ENcNaKr8yqZ","outputId":"7a7514c7-5f93-48ed-ca26-26d984654d11","trusted":true},"outputs":[{"data":{"text/plain":["True"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["torch.cuda.is_available()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:56:17.988122Z","iopub.status.busy":"2024-01-02T06:56:17.987217Z","iopub.status.idle":"2024-01-02T06:56:18.457123Z","shell.execute_reply":"2024-01-02T06:56:18.456305Z","shell.execute_reply.started":"2024-01-02T06:56:17.988084Z"},"id":"naGzU9fx5x50","trusted":true},"outputs":[],"source":["# Load training Dataset\n","with open('train.txt','r',encoding= 'utf-8') as file:\n","    train = file.read()"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-01-02T06:56:19.971036Z","iopub.status.busy":"2024-01-02T06:56:19.970673Z","iopub.status.idle":"2024-01-02T06:57:36.262942Z","shell.execute_reply":"2024-01-02T06:57:36.261863Z","shell.execute_reply.started":"2024-01-02T06:56:19.971008Z"},"id":"0KCqZUUC5x50","outputId":"450643f9-b680-4b12-cedc-c860a39ac717","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["163296 163296 105744\n"]}],"source":["sentences, tokenized_sentences, tokenized_sentences_with_diacritics,vocab = preprocessing(train)\n","print(len(sentences),len(tokenized_sentences), len(vocab))"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:57:39.721210Z","iopub.status.busy":"2024-01-02T06:57:39.720823Z","iopub.status.idle":"2024-01-02T06:57:47.657074Z","shell.execute_reply":"2024-01-02T06:57:47.656245Z","shell.execute_reply.started":"2024-01-02T06:57:39.721179Z"},"id":"bHZ3GTdJZQlr","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Bag Of Words\n"]}],"source":["tfIdf_model, features, words = featureExtraction(tokenized_sentences,sentences,vocab,featureType.BagOfWords)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-01-02T06:57:49.410906Z","iopub.status.busy":"2024-01-02T06:57:49.410377Z","iopub.status.idle":"2024-01-02T06:57:49.416570Z","shell.execute_reply":"2024-01-02T06:57:49.415576Z","shell.execute_reply.started":"2024-01-02T06:57:49.410865Z"},"id":"-xzUK7HRAhHH","outputId":"5716c6f9-1f62-4b70-a80a-772cc041be1d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(163296, 105713)\n"]}],"source":["print(features.shape)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-01-02T04:23:10.160215Z","iopub.status.busy":"2024-01-02T04:23:10.159768Z","iopub.status.idle":"2024-01-02T04:23:10.307032Z","shell.execute_reply":"2024-01-02T04:23:10.306184Z","shell.execute_reply.started":"2024-01-02T04:23:10.160171Z"},"id":"shAT_zT35x51","outputId":"96bf0a01-a144-4ec5-91cc-22f2f001cf76","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["145\n"]}],"source":["# get max sentence length\n","max_length = 0\n","lengthOfSentences = []\n","for sentence in sentences:\n","    lengthOfSentences.append(len(sentence))\n","    max_length = max(len(sentence),max_length)\n","\n","print(max_length)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:23:10.308522Z","iopub.status.busy":"2024-01-02T04:23:10.308120Z","iopub.status.idle":"2024-01-02T04:23:10.314552Z","shell.execute_reply":"2024-01-02T04:23:10.313735Z","shell.execute_reply.started":"2024-01-02T04:23:10.308434Z"},"id":"MafXkenE5x52","trusted":true},"outputs":[],"source":["# make letter to index\n","letter_to_code = {}\n","fixed_length = 6\n","with open('arabic_letters.pickle','rb') as file:\n","    letters = pickle.load(file,encoding='utf-8')\n","    letters = list(letters)\n","    # padding\n","    letters.append('$')\n","    letter_to_code = {item: index for index, item in enumerate(letters)}\n","#     for char, indx in letters_dict.items():\n","#         binary_representation = format(indx, f'0{fixed_length}b')\n","#         binary_representation = [int(i) for i in binary_representation]\n","#         # binary_representation.reverse()\n","#         letter_to_code[char] = binary_representation"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:23:10.316666Z","iopub.status.busy":"2024-01-02T04:23:10.316087Z","iopub.status.idle":"2024-01-02T04:23:10.327828Z","shell.execute_reply":"2024-01-02T04:23:10.327034Z","shell.execute_reply.started":"2024-01-02T04:23:10.316639Z"},"id":"p0ynDhEQ5x53","trusted":true},"outputs":[],"source":["with open('diacritic2id.pickle','rb') as file:\n","    diacritics = pickle.load(file,encoding='utf-8')"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:23:10.329317Z","iopub.status.busy":"2024-01-02T04:23:10.329061Z","iopub.status.idle":"2024-01-02T04:23:12.153392Z","shell.execute_reply":"2024-01-02T04:23:12.152376Z","shell.execute_reply.started":"2024-01-02T04:23:10.329292Z"},"id":"ConWFUPg5x53","trusted":true},"outputs":[],"source":["sentences_indexer = wordIndexer(tokenized_sentences,vocab)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:23:12.155403Z","iopub.status.busy":"2024-01-02T04:23:12.155015Z","iopub.status.idle":"2024-01-02T04:23:30.492057Z","shell.execute_reply":"2024-01-02T04:23:30.491250Z","shell.execute_reply.started":"2024-01-02T04:23:12.155350Z"},"id":"WynREE1a5x53","trusted":true},"outputs":[],"source":["sentences_char, sentences_diacritics = charIndexer(tokenized_sentences_with_diacritics,max_length,diacritics,letter_to_code)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T04:23:30.493433Z","iopub.status.busy":"2024-01-02T04:23:30.493128Z","iopub.status.idle":"2024-01-02T04:23:46.626257Z","shell.execute_reply":"2024-01-02T04:23:46.625231Z","shell.execute_reply.started":"2024-01-02T04:23:30.493406Z"},"id":"9T3DTwqr5x54","trusted":true},"outputs":[],"source":["train_dataset = ArabicDataset(len(vocab)+3,sentences_indexer, sentences_char, sentences_diacritics)"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:46:15.545684Z","iopub.status.busy":"2024-01-02T06:46:15.545318Z","iopub.status.idle":"2024-01-02T06:46:26.667885Z","shell.execute_reply":"2024-01-02T06:46:26.666793Z","shell.execute_reply.started":"2024-01-02T06:46:15.545655Z"},"id":"llIGbBLb6F0C","trusted":true},"outputs":[],"source":["train_dataset = ArabicDatasetTFIDF(tokenized_sentences, features, sentences_char, sentences_diacritics, words, max_length)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-01-02T06:58:01.489480Z","iopub.status.busy":"2024-01-02T06:58:01.488450Z","iopub.status.idle":"2024-01-02T06:58:01.853408Z","shell.execute_reply":"2024-01-02T06:58:01.852454Z","shell.execute_reply.started":"2024-01-02T06:58:01.489443Z"},"id":"i3x1EGN05x54","outputId":"b839aecb-3eb5-4ad3-9c5e-c0bbaf1be2d5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["ArabicDiacritization(\n","  (embedding_word): Embedding(105747, 300)\n","  (lstm_words): LSTM(300, 256, num_layers=3, batch_first=True, dropout=0.1, bidirectional=True)\n","  (embedding_char): Embedding(37, 70)\n","  (lstm_chars): LSTM(70, 256, num_layers=3, batch_first=True, dropout=0.1, bidirectional=True)\n","  (linear): Linear(in_features=512, out_features=15, bias=True)\n",")\n"]}],"source":["model = ArabicDiacritization(vocab_size=len(vocab)+3, embedding_dim=300, hidden_size=256, num_layers=3, dropout = 0.1, n_classes=15)\n","print(model)"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-01-02T05:23:15.127865Z","iopub.status.busy":"2024-01-02T05:23:15.127481Z","iopub.status.idle":"2024-01-02T05:45:17.645130Z","shell.execute_reply":"2024-01-02T05:45:17.644180Z","shell.execute_reply.started":"2024-01-02T05:23:15.127834Z"},"id":"W7fNOGLw5x55","outputId":"58e2fe2d-21e9-40aa-df3b-6b05f11658cd","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cuda\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 319/319 [02:11<00:00,  2.43it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 1 | Train Loss: 0.0012200954370200634         | Train Accuracy: 0.7881250480453879\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 319/319 [02:12<00:00,  2.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 2 | Train Loss: 0.00042389988084323704         | Train Accuracy: 0.9277684740353743\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 319/319 [02:12<00:00,  2.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 3 | Train Loss: 0.00033702413202263415         | Train Accuracy: 0.9421607768110027\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 319/319 [02:11<00:00,  2.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 4 | Train Loss: 0.00029877739143557847         | Train Accuracy: 0.9484056594533327\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 319/319 [02:11<00:00,  2.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 5 | Train Loss: 0.00027823366690427065         | Train Accuracy: 0.951869716953095\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 319/319 [02:12<00:00,  2.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 6 | Train Loss: 0.0002648368536029011         | Train Accuracy: 0.9541973289039377\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 319/319 [02:12<00:00,  2.41it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 7 | Train Loss: 0.0002565908362157643         | Train Accuracy: 0.9554805748156194\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 319/319 [02:12<00:00,  2.40it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 8 | Train Loss: 0.0002526135358493775         | Train Accuracy: 0.956133034176705\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 319/319 [02:11<00:00,  2.42it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epochs: 9 | Train Loss: 0.000250633544055745         | Train Accuracy: 0.956419211066592\n","\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 319/319 [02:12<00:00,  2.40it/s]"]},{"name":"stdout","output_type":"stream","text":["Epochs: 10 | Train Loss: 0.00024814880453050137         | Train Accuracy: 0.9568590134584561\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["training(model, train_dataset, letter_to_code['$'])"]},{"cell_type":"markdown","metadata":{},"source":["# Save the Trained Model"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:35:15.372602Z","iopub.status.busy":"2024-01-02T06:35:15.372248Z","iopub.status.idle":"2024-01-02T06:35:15.672609Z","shell.execute_reply":"2024-01-02T06:35:15.671696Z","shell.execute_reply.started":"2024-01-02T06:35:15.372574Z"},"id":"OaH2ZNH55x55","trusted":true},"outputs":[],"source":["torch.save(model.state_dict(), 'model_state_dict.pt')"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:35:17.638982Z","iopub.status.busy":"2024-01-02T06:35:17.638438Z","iopub.status.idle":"2024-01-02T06:35:17.931453Z","shell.execute_reply":"2024-01-02T06:35:17.930439Z","shell.execute_reply.started":"2024-01-02T06:35:17.638944Z"},"id":"uyZSme_dBWv6","trusted":true},"outputs":[],"source":["torch.save(model, 'model.pt')"]},{"cell_type":"markdown","metadata":{},"source":["# Evaluation"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2024-01-02T06:21:17.508911Z","iopub.status.busy":"2024-01-02T06:21:17.508041Z","iopub.status.idle":"2024-01-02T06:21:17.520640Z","shell.execute_reply":"2024-01-02T06:21:17.519607Z","shell.execute_reply.started":"2024-01-02T06:21:17.508874Z"},"id":"lm0OxTfsXP7Y","trusted":true},"outputs":[],"source":["def evaluate(model, test_dataset, pad,batch_size=512):\n","\n","  # Create a DataLoader for the test dataset\n","  test_dataloader = DataLoader(test_dataset,batch_size=batch_size)\n","\n","  # Check if CUDA (GPU) is available\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","  if use_cuda:\n","    # Move the model to GPU if available\n","    model = model.cuda()\n","\n","  # Convert the padding value to a tensor and move it to the appropriate device\n","  pad = torch.tensor(pad)\n","  pad = pad.to(device)\n","\n","  # Variables to store evaluation metrics\n","  total_acc_test = 0\n","  total_predection = 0\n","\n","  # Tensor to store test predictions\n","  test_predections = torch.tensor([])\n","  test_predections = test_predections.to(device)\n","\n","  # Disable gradient computation during evaluation\n","  with torch.no_grad():\n","\n","     # Loop through batches in the test dataloader\n","    for sentences_indexer, sentences_char, sentences_diacritics in tqdm(test_dataloader):\n","\n","      # Move data to the appropriate device (CPU or GPU)\n","      sentences_indexer = sentences_indexer.to(device)\n","      sentences_char = sentences_char.to(device)\n","      sentences_diacritics = sentences_diacritics.to(device)\n","\n","      # Forward pass\n","      output = model(sentences_indexer,sentences_char)\n","      output = output.to(device)\n","    \n","      # Reshape tensors for calculation of accuracy\n","      sentences_char = sentences_char.view(-1)\n","      output = output.view(-1, output.size(-1))\n","      sentences_diacritics = sentences_diacritics.view(-1)\n","\n","      # Apply mask to exclude padding values\n","      mask = sentences_char != pad\n","      sentences_char = sentences_char[mask]\n","      output = output[mask]\n","      sentences_diacritics = sentences_diacritics[mask]\n","\n","      # Get the index of the maximum value as the predicted class\n","      output = torch.argmax(output,dim=1)\n","\n","      # Calculate batch accuracy\n","      acc = (sentences_diacritics == output).sum().item()\n","      total_acc_test += acc\n","      total_predection += sentences_diacritics.size(0)\n","\n","      # Concatenate the predictions to the tensor\n","      test_predections = torch.cat((test_predections, output))\n","\n","    # Calculate overall accuracy\n","    total_acc_test /= total_predection\n","    print(total_acc_test)\n","\n","  # Convert predictions to int64 and move to CPU\n","  test_predections = test_predections.to(torch.int64)\n","  test_predections = test_predections.to(torch.device('cpu'))\n","  \n","  # Create a DataFrame to store predictions\n","  df = pd.DataFrame({'ID': range(len(test_predections)), 'label': test_predections})\n","\n","  # Save predictions to a CSV file\n","  csv_file_path = 'predections.csv'\n","  df.to_csv(csv_file_path, index=False)\n","\n","  # Print accuracy and return the test predections\n","  print(f'\\nTest Accuracy: {total_acc_test}')\n","  return test_predections"]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-01-02T06:21:20.941565Z","iopub.status.busy":"2024-01-02T06:21:20.940682Z","iopub.status.idle":"2024-01-02T06:21:21.172736Z","shell.execute_reply":"2024-01-02T06:21:21.171859Z","shell.execute_reply.started":"2024-01-02T06:21:20.941527Z"},"id":"NMGzRWPPXrKk","outputId":"e3260f1c-1c84-4d56-bb1c-a4810a866303","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["402 402 2694\n","121\n"]}],"source":["with open('val.txt','r',encoding= 'utf-8') as file:\n","    val = file.read()\n","val_sentences, val_tokenized_sentences, val_tokenized_sentences_with_diacritics, val_vocab = preprocessing(val)\n","print(len(val_sentences),len(val_tokenized_sentences), len(val_vocab))\n","val_max_length = 0\n","for val_sentence in val_sentences:\n","    val_max_length = max(len(val_sentence),val_max_length)\n","print(val_max_length)\n","val_sentences_indexer = wordIndexer(val_tokenized_sentences,val_vocab)\n","val_sentences_char, val_sentences_diacritics = charIndexer(val_tokenized_sentences_with_diacritics,val_max_length,diacritics,letter_to_code)\n","val_dataset = ArabicDataset(len(vocab)+3,val_sentences_indexer, val_sentences_char, val_sentences_diacritics)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2024-01-02T06:21:32.496515Z","iopub.status.busy":"2024-01-02T06:21:32.496064Z","iopub.status.idle":"2024-01-02T06:21:32.673140Z","shell.execute_reply":"2024-01-02T06:21:32.672255Z","shell.execute_reply.started":"2024-01-02T06:21:32.496477Z"},"id":"UFzvNN3ga5Vx","outputId":"a7602115-973d-4f83-c5bf-a8617a018b5d","trusted":true},"outputs":[],"source":["test_predections = evaluate(model,val_dataset,letter_to_code['$'])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":7403259,"sourceId":66711,"sourceType":"competition"},{"datasetId":4246808,"sourceId":7318192,"sourceType":"datasetVersion"},{"datasetId":4246821,"sourceId":7318211,"sourceType":"datasetVersion"},{"datasetId":4246906,"sourceId":7318723,"sourceType":"datasetVersion"},{"datasetId":4248864,"sourceId":7321475,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
